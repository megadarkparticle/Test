{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Lab 3 - Word Segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/megadarkparticle/Test/blob/master/PyTorch_Lab_3_Word_Segmentation.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "_iRdVX6c5mGt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PyTorch Lab 3\n",
        "\n",
        "Written by Prachya Boonkwan (Arm)"
      ]
    },
    {
      "metadata": {
        "id": "kc_wJrf85nT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "604fa5c8-56ca-4212-9d71-00896c532931"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl torchvision tqdm nltk\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "ucZjh7KI5mGw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sequence Prediction"
      ]
    },
    {
      "metadata": {
        "id": "6DRFWtF19pA9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this workshop, we are going to implement a word tokenizer based on recurrent neural networks.\n",
        "\n",
        "We will apply sequence prediction to word segmentation. We are now going to simulate the problem in English by removing all spaces in English and pretending that it is a consecutively written language. For example, a sentence like\n",
        "\n",
        "`Hi there. My name is Arm.`\n",
        "\n",
        "would be converted into a string similar to below:\n",
        "\n",
        "`hithere.mynameisarm.`\n",
        "\n",
        "Then we will tokenize this string with a deep neural network model and compare the result with the original string."
      ]
    },
    {
      "metadata": {
        "id": "gb8uytls5mG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Header"
      ]
    },
    {
      "metadata": {
        "id": "82xNj64z5mG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This part is the header of the code. My favorite import aliases for PyTorch are as follows. This will be very useful for speed coding."
      ]
    },
    {
      "metadata": {
        "id": "OB_zhMoX5mG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "#-*- coding: utf-8 -*-\n",
        "\n",
        "import torch as T\n",
        "import torch.nn as N\n",
        "import torch.optim as O\n",
        "\n",
        "from tqdm import tqdm    # Nice progressbar\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0DC9hNTK5mG-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-----"
      ]
    },
    {
      "metadata": {
        "id": "s04Bqg_75mG_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "metadata": {
        "id": "S8zpApCh5mHB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using Free Dataset from The Gutenberg Project "
      ]
    },
    {
      "metadata": {
        "id": "ySC05reSOGXK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will download a sample text from NLTK (natural language toolkit) and make it our dataset (a.k.a. *corpus*). Here we choose 'The Hamlet' of Shakespeare, a classic English novel from the 16th century. We convert every word into the lower case (e.g. `Hello` -> `hello`).\n",
        "\n",
        "Note that this dataset is textual, not numerical. We will have to convert it into a bunch of numbers so that it can be processed by the neural networks."
      ]
    },
    {
      "metadata": {
        "id": "6u9RoHx25mHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ec295171-bd0c-46f5-90ed-51172fabbade"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "corpus = [ [word.lower() for word in sent]\n",
        "           for sent in gutenberg.sents('shakespeare-hamlet.txt') ]\n",
        "\n",
        "crplen = len(corpus)\n",
        "print('Number of sentences = {}'.format(crplen))\n",
        "\n",
        "no_words = sum(len(sent) for sent in corpus)\n",
        "print('Number of words     = {}'.format(no_words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences = 3106\n",
            "Number of words     = 37360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZZWpry1N5mHM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Character Indexing"
      ]
    },
    {
      "metadata": {
        "id": "xZaqs0gcOq3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we obtain the dataset, we will now convert it into sequences of numbers. Our objective is to convert a string e.g. `hello` into a vector of five numbers representing each character. Such numbers are called *character indices*.\n",
        "\n",
        "We first populate all characters in the dataset into `all_chars`."
      ]
    },
    {
      "metadata": {
        "id": "Fht2AxNL5mHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_chars = set()\n",
        "for sent in corpus:\n",
        "    for word in sent:\n",
        "        all_chars.update(set(word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tje9OYqnPmYy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we create a mapping table from an index to a corresponding character. Let's name this table `idx2char`."
      ]
    },
    {
      "metadata": {
        "id": "QbWbBcu45mHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "810b85a8-fd3d-4235-fa1f-1d42d64c663b"
      },
      "cell_type": "code",
      "source": [
        "idx2char = [None] + sorted(all_chars)\n",
        "print(idx2char)\n",
        "\n",
        "no_chars = len(idx2char)\n",
        "print('Number of characters = {}'.format(no_chars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, '!', '&', \"'\", '(', ')', ',', '-', '.', '1', '5', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Number of characters = 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NQzAaGbrPtz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we create another table that maps each character back to its corresponding index. Let's name this table `char2idx`."
      ]
    },
    {
      "metadata": {
        "id": "yxDKPXf-5mHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "1bc3dfa1-b778-4b56-ffa6-4b07e98aabba"
      },
      "cell_type": "code",
      "source": [
        "char2idx = {}\n",
        "for (idx, char) in enumerate(idx2char):\n",
        "    char2idx[char] = idx\n",
        "print(char2idx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{None: 0, '!': 1, '&': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '1': 9, '5': 10, '9': 11, ':': 12, ';': 13, '?': 14, '[': 15, ']': 16, 'a': 17, 'b': 18, 'c': 19, 'd': 20, 'e': 21, 'f': 22, 'g': 23, 'h': 24, 'i': 25, 'j': 26, 'k': 27, 'l': 28, 'm': 29, 'n': 30, 'o': 31, 'p': 32, 'q': 33, 'r': 34, 's': 35, 't': 36, 'u': 37, 'v': 38, 'w': 39, 'x': 40, 'y': 41, 'z': 42}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aI-IAZ9jQGmY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that we deliberately add `None` as one of the characters. It will be used for finding an unknown/unseen character to an index by mapping it to `None`."
      ]
    },
    {
      "metadata": {
        "id": "XTb18K4h5mHd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conversion String and Index Sequence"
      ]
    },
    {
      "metadata": {
        "id": "hpVm7EmfP8QC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is now very easy to convert a character sequence into a sequence of character index. Here we sequentially convert each character in the sequence into its corresponding index."
      ]
    },
    {
      "metadata": {
        "id": "I4RM0ONl5mHe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def str2idxseq(charseq):\n",
        "    idxseq = []\n",
        "    for char in charseq:\n",
        "        char = char.lower()\n",
        "        if char in char2idx:\n",
        "            idxseq.append(char2idx[char])\n",
        "        else:\n",
        "            idxseq.append(char2idx[None])\n",
        "    return idxseq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oq6z-jv5mHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7380361-dfa6-4851-e282-012070faecf2"
      },
      "cell_type": "code",
      "source": [
        "cuter_idxseq = str2idxseq('CutEr')\n",
        "print(cuter_idxseq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19, 37, 36, 21, 34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KFpqcWLYQhdt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Also, we can sequentially convert a sequence of character indices back to a character sequence. Note that any unknown character index will be converted to a space."
      ]
    },
    {
      "metadata": {
        "id": "40RXwSPf5mHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def idxseq2str(idxseq):\n",
        "    charseq = []\n",
        "    for idx in idxseq:\n",
        "        if idx < len(idx2char):\n",
        "            charseq.append(idx2char[idx])\n",
        "        else:\n",
        "            charseq.append(' ')\n",
        "    return charseq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbmo_Sg_5mHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27930bf7-c1b3-489b-d947-f138fcae838b"
      },
      "cell_type": "code",
      "source": [
        "print(idxseq2str(cuter_idxseq))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['c', 'u', 't', 'e', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-P8vK6Mc5mHu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preparing Training and Testing Sets"
      ]
    },
    {
      "metadata": {
        "id": "TO26iYOsQ2Yc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now it's time to prepare our dataset for the neural networks. We will convert a sequence of words into a representation containing a sequence of character indices and a sequence of word delimiters. For example, the list `['my', 'name']` will be converted into:\n",
        "\n",
        "- A sequence of character indices [`m`, `y`, `n`, `a`, `m`, `e`]\n",
        "- A sequence of word delimiters $[\\mathbb{F}, \\mathbb{T}, \\mathbb{F}, \\mathbb{F}, \\mathbb{F}, \\mathbb{T}]$\n",
        "\n",
        "| m | y | n | a | m | e |\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "| F | T | F | F | F | T |"
      ]
    },
    {
      "metadata": {
        "id": "AVaB1wj45mHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sent2data(sent):\n",
        "    charidxs = []\n",
        "    wordbrks = []\n",
        "    for charseq in sent:\n",
        "        idxs = str2idxseq(charseq)\n",
        "        charidxs.extend(idxs)\n",
        "        wordbrks.extend((len(idxs) - 1) * [False] + [True])\n",
        "    return (charidxs, wordbrks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjX34HbefK7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try it out with the list `['hello', 'world']`."
      ]
    },
    {
      "metadata": {
        "id": "9E0PwaB05mHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dbf29f95-4de9-4c96-c877-e2361af4a84f"
      },
      "cell_type": "code",
      "source": [
        "sent = ['hello', 'world']\n",
        "charidxs, wordbrks = sent2data(sent)\n",
        "print(charidxs)\n",
        "print(wordbrks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[24, 21, 28, 28, 31, 39, 31, 34, 28, 20]\n",
            "[False, False, False, False, True, False, False, False, False, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxESG06yRxAS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we convert the whole dataset into this representation."
      ]
    },
    {
      "metadata": {
        "id": "vie67KpU5mH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def corpus2dataset(corpus):\n",
        "    dataset = []\n",
        "    for sent in corpus:\n",
        "        charidxs, wordbrks = sent2data(sent)\n",
        "        dataset.append((charidxs, wordbrks))\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZHPL1iDA5mH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "e93e8954-0921-40c2-9970-e0aa3f820526"
      },
      "cell_type": "code",
      "source": [
        "crp = [ ['hello', 'world'],\n",
        "        ['my', 'name', 'is', 'arm'] ]\n",
        "ds = corpus2dataset(crp)\n",
        "print(ds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[([24, 21, 28, 28, 31, 39, 31, 34, 28, 20], [False, False, False, False, True, False, False, False, False, True]), ([29, 41, 30, 17, 29, 21, 25, 35, 17, 34, 29], [False, True, False, False, False, True, False, True, False, False, True])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oMXn2qt_R9BF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FInally, we separate the dataset into two parts: the training set (80%), and the testing set (20%)."
      ]
    },
    {
      "metadata": {
        "id": "xY07mHr95mH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0c52562-c37a-49f6-8f84-01823751a762"
      },
      "cell_type": "code",
      "source": [
        "training_len = int(crplen * 0.8)           # 80% for training, 20% for testing\n",
        "training_set = corpus2dataset(corpus[: training_len])\n",
        "testing_set = corpus2dataset(corpus[training_len :])\n",
        "print('Size of training set = {}'.format(len(training_set)))\n",
        "print('Size of testing set = {}'.format(len(testing_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set = 2484\n",
            "Size of testing set = 622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cAPfjULx5mH_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-----"
      ]
    },
    {
      "metadata": {
        "id": "-D1SloW2SkVv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Common Settings"
      ]
    },
    {
      "metadata": {
        "id": "0rmIYHXYHQom",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dim_charvec = 10\n",
        "dim_trans = 5\n",
        "no_rnn_layers = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gw7RHXAqSiq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-------"
      ]
    },
    {
      "metadata": {
        "id": "xscQzhtz5mIA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Embedding Layer"
      ]
    },
    {
      "metadata": {
        "id": "E8hyjjUaSMFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Embedding layer is used for mapping any sparse item, such as characters, words, and any symbols, into a vector. This vector is sometimes called *embedding vector*. The command `N.Embedding` creates an embedding layer containing a specified number of sparse items and their embedding vectors contain a specified number of elements. "
      ]
    },
    {
      "metadata": {
        "id": "f64Wzpj-5mIB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "729f8791-e220-4cda-9aea-65e82bbec71d"
      },
      "cell_type": "code",
      "source": [
        "charemb_layer = N.Embedding(no_chars, dim_charvec)\n",
        "print(charemb_layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding(43, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MYTzSOcTTC8p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is how we compute an embedding vector of each character in a string."
      ]
    },
    {
      "metadata": {
        "id": "HLQJU0VO5mIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "10238053-b9e7-4e9d-a82c-cbd27549a1db"
      },
      "cell_type": "code",
      "source": [
        "charseq = 'hello'\n",
        "\n",
        "charidxs = str2idxseq(charseq)\n",
        "print('Index sequence of \"{}\" is {}\\n'.format(charseq, charidxs))\n",
        "\n",
        "charvecs = charemb_layer(T.LongTensor(charidxs))\n",
        "print('Embedding vectors for each character:\\n{}'.format(charvecs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index sequence of \"hello\" is [24, 21, 28, 28, 31]\n",
            "\n",
            "Embedding vectors for each character:\n",
            "tensor([[-0.5247,  0.8452,  0.1333, -1.1519, -0.1654,  0.2099, -0.7905, -0.0108,\n",
            "         -0.5143,  2.3076],\n",
            "        [ 0.1654, -1.8094, -0.1885, -0.3151,  0.8453, -2.0892,  0.0273,  1.0842,\n",
            "         -0.2621,  0.5427],\n",
            "        [-0.1242,  0.6750, -1.5195,  0.5411,  0.8821,  0.1581,  0.7997, -0.3138,\n",
            "         -0.7617,  0.9607],\n",
            "        [-0.1242,  0.6750, -1.5195,  0.5411,  0.8821,  0.1581,  0.7997, -0.3138,\n",
            "         -0.7617,  0.9607],\n",
            "        [ 0.7349, -1.0743, -1.2035,  1.9856,  0.5558, -0.9231, -0.8533,  1.6741,\n",
            "         -1.4180, -1.5226]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5V27cWkXTKyo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that the embedding vector of two `l`'s are the same."
      ]
    },
    {
      "metadata": {
        "id": "sdxDJUnkTTQY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Although the result is a matrix, we can easily get access to each embedding vector by matrix indexing `[]`."
      ]
    },
    {
      "metadata": {
        "id": "rU3hXAKp5mIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d9e8537e-9742-4026-b8d3-77cb7efc2781"
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(charseq)):\n",
        "    print('Character embedding for index {} \"{}\" is:\\n{}'.format(i, charseq[i], charvecs[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Character embedding for index 0 \"h\" is:\n",
            "tensor([-0.5247,  0.8452,  0.1333, -1.1519, -0.1654,  0.2099, -0.7905, -0.0108,\n",
            "        -0.5143,  2.3076], grad_fn=<SelectBackward>)\n",
            "Character embedding for index 1 \"e\" is:\n",
            "tensor([ 0.1654, -1.8094, -0.1885, -0.3151,  0.8453, -2.0892,  0.0273,  1.0842,\n",
            "        -0.2621,  0.5427], grad_fn=<SelectBackward>)\n",
            "Character embedding for index 2 \"l\" is:\n",
            "tensor([-0.1242,  0.6750, -1.5195,  0.5411,  0.8821,  0.1581,  0.7997, -0.3138,\n",
            "        -0.7617,  0.9607], grad_fn=<SelectBackward>)\n",
            "Character embedding for index 3 \"l\" is:\n",
            "tensor([-0.1242,  0.6750, -1.5195,  0.5411,  0.8821,  0.1581,  0.7997, -0.3138,\n",
            "        -0.7617,  0.9607], grad_fn=<SelectBackward>)\n",
            "Character embedding for index 4 \"o\" is:\n",
            "tensor([ 0.7349, -1.0743, -1.2035,  1.9856,  0.5558, -0.9231, -0.8533,  1.6741,\n",
            "        -1.4180, -1.5226], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Sh4d5SI5mIM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-----"
      ]
    },
    {
      "metadata": {
        "id": "SyAvgNBy5mIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "MyhKi4KpTvKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A recurrent neural network (RNN) is a neural network that takes its internal state as an input of the next feed-forwarding. Doing so enables the neural network to capture long-range dependency of any input sequence, making sequence prediction possible."
      ]
    },
    {
      "metadata": {
        "id": "6LSHC77o5mIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Common Parameters"
      ]
    },
    {
      "metadata": {
        "id": "aBFMjUDo5mIO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dim_charvec = 10\n",
        "dim_trans = 5\n",
        "no_rnn_layers = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NuG-yD135mIQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Unidirectional RNNs"
      ]
    },
    {
      "metadata": {
        "id": "_fNT5M0BUOD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The simplest form of RNNs is a unidirectional RNN. It takes a sequence of sparse items as an input, sweeps from left to right, performs feed-forwarding, and produces two outputs:\n",
        "\n",
        "1. The context vector for each step of feed-forwarding\n",
        "2. The last internal state vector of the RNN's cell\n",
        "\n",
        "In this example, we choose Gated Recurrent Unit (GRU) `N.GRU`. You can also replace it with Long Short-Term Memories (LSTM) `N.LSTM`. They are almost the same, but the first converges a bit faster."
      ]
    },
    {
      "metadata": {
        "id": "TJtLIvdQ5mIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8b8dc9c-3e73-4d0c-cd06-b610c0097483"
      },
      "cell_type": "code",
      "source": [
        "rnn_unidir = N.GRU(dim_charvec, dim_trans, no_rnn_layers)\n",
        "print(rnn_unidir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRU(10, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LldNTkyOU-t3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To perform sequence learning, we first convert a sequence of character embedding vectors into a mini-batch with the command `unsqueeze(1)`. The argument `1` is the dimension that PyTorch's RNNs consider as the mini-batch index.\n",
        "\n",
        "Then we pass this mini-batch into an RNN. It will produce two outputs:\n",
        "\n",
        "1. The context vectors of each step of feed-forwarding\n",
        "2. The last internal state vector of the RNN's cell\n",
        "\n",
        "Once we obtain the outputs, we finally remove the mini-batch by the command `squeeze(1)`, which eliminates the mini-batch dimension from the obtained tensor."
      ]
    },
    {
      "metadata": {
        "id": "2tfCYde75mIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "76aab17d-1661-4eec-fb2c-4d60d9478a32"
      },
      "cell_type": "code",
      "source": [
        "ctxvecs_unidir, lasthids_unidir = rnn_unidir(charvecs.unsqueeze(1))\n",
        "ctxvecs_unidir, lasthids_unidir = ctxvecs_unidir.squeeze(1), lasthids_unidir.squeeze(1)\n",
        "print(ctxvecs_unidir)\n",
        "print(lasthids_unidir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2450, -0.2090, -0.1965, -0.0059, -0.3642],\n",
            "        [-0.4945, -0.1527, -0.1515,  0.0430,  0.1381],\n",
            "        [-0.1123, -0.3703, -0.6303, -0.2080, -0.3182],\n",
            "        [ 0.0211, -0.5101, -0.7938, -0.4297, -0.5919],\n",
            "        [-0.4549, -0.5775, -0.6892, -0.4372, -0.1981]],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "tensor([[-0.4549, -0.5775, -0.6892, -0.4372, -0.1981]], grad_fn=<SqueezeBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KkoESOlz5mIY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bidirectional RNNs"
      ]
    },
    {
      "metadata": {
        "id": "tiIL-WnBWFpz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can sweep the input sequence from both directions and perform feed-forwarding separately. Just specify the argument `bidirection=True` in the command `N.GRU`."
      ]
    },
    {
      "metadata": {
        "id": "aNL7O-k45mIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98fe1186-581f-4a71-8219-92619938cf47"
      },
      "cell_type": "code",
      "source": [
        "rnn_bidir = N.GRU(dim_charvec, dim_trans, no_rnn_layers, bidirectional=True)\n",
        "print(rnn_bidir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRU(10, 5, bidirectional=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rLg0HCSfWsCy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that the context vectors double the size. That is because they contains one half produced by sweeping from left to right and the other half produced by sweeping from right to left."
      ]
    },
    {
      "metadata": {
        "id": "0CDzXkjG5mIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "971c5164-0c22-4f57-c407-6b7f34c48081"
      },
      "cell_type": "code",
      "source": [
        "ctxvecs_bidir, lasthids_bidir = rnn_bidir(charvecs.unsqueeze(1))\n",
        "ctxvecs_bidir, lasthids_bidir = ctxvecs_bidir.squeeze(1), lasthids_bidir.squeeze(1)\n",
        "print(ctxvecs_bidir)\n",
        "print(lasthids_bidir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1077, -0.0754,  0.3038,  0.0512, -0.4378,  0.3669, -0.1486,  0.3601,\n",
            "          0.1760, -0.5056],\n",
            "        [-0.3643, -0.2709,  0.2761, -0.4367,  0.3391,  0.5013,  0.5350,  0.7360,\n",
            "          0.5908, -0.4669],\n",
            "        [-0.4253, -0.0661,  0.7566, -0.5066,  0.7125,  0.4755,  0.3775,  0.5880,\n",
            "          0.5998, -0.3178],\n",
            "        [-0.4808,  0.0592,  0.8763, -0.5267,  0.8272,  0.4816,  0.2830,  0.5846,\n",
            "          0.5979, -0.3492],\n",
            "        [ 0.4272,  0.3407,  0.3587, -0.9223,  0.9628,  0.4963,  0.1298,  0.5965,\n",
            "          0.5822, -0.3750]], grad_fn=<SqueezeBackward1>)\n",
            "tensor([[ 0.4272,  0.3407,  0.3587, -0.9223,  0.9628],\n",
            "        [ 0.3669, -0.1486,  0.3601,  0.1760, -0.5056]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WkWENkgz5mIg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multilayer Bidirectional RNNs"
      ]
    },
    {
      "metadata": {
        "id": "41_ZrzJ_XCkG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also specify the number of RNN layers for learning more abstract representation, e.g. character -> syllable -> morpheme -> word -> phrase -> sentence -> etc."
      ]
    },
    {
      "metadata": {
        "id": "X4jFKXMT5mIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebf89682-00b9-498f-b439-fd9b88ef887f"
      },
      "cell_type": "code",
      "source": [
        "rnn_bidir_layer2 = N.GRU(dim_charvec, dim_trans, 2, bidirectional=True)\n",
        "print(rnn_bidir_layer2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRU(10, 5, num_layers=2, bidirectional=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XIAESULgXZSZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that the context vectors produced by the multilayer RNNs belong to the last layer. However, the last state vectors obtained contain all vectors of the layers."
      ]
    },
    {
      "metadata": {
        "id": "wXmbu49s5mIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c0da0c2c-cecf-426b-c877-c565a19746a5"
      },
      "cell_type": "code",
      "source": [
        "ctxvecs_bidir_layer2, lasthids_bidir_layer2 = rnn_bidir_layer2(charvecs.unsqueeze(1))\n",
        "ctxvecs_bidir_layer2, lasthids_bidir_layer2 = ctxvecs_bidir_layer2.squeeze(1), lasthids_bidir_layer2.squeeze(1)\n",
        "print(ctxvecs_bidir_layer2)\n",
        "print(lasthids_bidir_layer2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1019, -0.0873, -0.1730,  0.2718,  0.1579,  0.4677, -0.3226, -0.5284,\n",
            "         -0.7485,  0.1720],\n",
            "        [ 0.1702, -0.1418, -0.2438,  0.3850,  0.0683,  0.5322, -0.3891, -0.5265,\n",
            "         -0.7490,  0.1985],\n",
            "        [ 0.0478, -0.2204, -0.2051,  0.4333, -0.2654,  0.3844, -0.3585, -0.4712,\n",
            "         -0.7157,  0.0332],\n",
            "        [ 0.0017, -0.1692, -0.1522,  0.4403, -0.4168,  0.2767, -0.2824, -0.3847,\n",
            "         -0.5417,  0.1335],\n",
            "        [ 0.0444, -0.1194, -0.1331,  0.3934, -0.3437,  0.1401, -0.1665, -0.2381,\n",
            "         -0.2697,  0.3033]], grad_fn=<SqueezeBackward1>)\n",
            "tensor([[-0.6572, -0.1956, -0.0087, -0.0497,  0.0558],\n",
            "        [ 0.1348,  0.3058, -0.4125,  0.4060, -0.1338],\n",
            "        [ 0.0444, -0.1194, -0.1331,  0.3934, -0.3437],\n",
            "        [ 0.4677, -0.3226, -0.5284, -0.7485,  0.1720]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I7XBCA2C5mIl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-------"
      ]
    },
    {
      "metadata": {
        "id": "dnFUzVET5mIm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word Segmentation"
      ]
    },
    {
      "metadata": {
        "id": "-8pG7rldX2vr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now observe an application of sequence prediction --- word segmentation."
      ]
    },
    {
      "metadata": {
        "id": "b7RNd3205mIm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Neural Architecture"
      ]
    },
    {
      "metadata": {
        "id": "bzAEf1fsYEVj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The neural model for word segmentation used in this workshop is simple and practical. The neural architecture is as follows.\n",
        "\n",
        "1. Character embedding layer\n",
        "2. Multilayer bidirectional RNNs\n",
        "3. Hyperbolic tangent `N.Tanh`\n",
        "4. Linear layer (hidden layer)\n",
        "5. (Log) Softmax `N.LogSoftmax(dim=1)`"
      ]
    },
    {
      "metadata": {
        "id": "2Z0kSAkv5mIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WordsegModel(N.Module):\n",
        "    \n",
        "    def __init__(self, dim_charvec, dim_trans, no_layers):\n",
        "        super(WordsegModel, self).__init__()\n",
        "        self._dim_charvec = dim_charvec\n",
        "        self._dim_trans = dim_trans\n",
        "        self._no_layers = no_layers\n",
        "        \n",
        "        self._charemb_layer = N.Embedding(no_chars, self._dim_charvec)\n",
        "        self._rnn = N.GRU(self._dim_charvec, self._dim_trans, self._no_layers, bidirectional=True)\n",
        "        self._tanh = N.Tanh()\n",
        "        self._hidden_layer = N.Linear(2 * self._dim_trans, 2)    # Predicting two classes: break / no break\n",
        "        self._log_softmax = N.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, charidxs):\n",
        "        try:\n",
        "            charvecs = self._charemb_layer(T.LongTensor(charidxs))\n",
        "            # print('charvecs =\\n{}'.format(charvecs))\n",
        "            ctxvecs, lasthids = self._rnn(charvecs.unsqueeze(1))\n",
        "            ctxvecs, lasthids = ctxvecs.squeeze(1), lasthids.squeeze(1)\n",
        "            # print('ctxvecs =\\n{}'.format(ctxvecs))\n",
        "            statevecs = self._hidden_layer(self._tanh(ctxvecs))\n",
        "            # print('statevecs =\\n{}'.format(statevecs))\n",
        "            brkvecs = self._log_softmax(statevecs)\n",
        "            # print('brkvecs =\\n{}'.format(brkvecs))\n",
        "            return brkvecs\n",
        "        except RuntimeError:\n",
        "            raise RuntimeError(statevecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wa-hEYP5gI5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's create a word segmentation model with the following configuration.\n",
        "\n",
        "- Dimension of character embedding = 16\n",
        "- Dimension of GRU's transition vector = 32\n",
        "- Number of GRU layers = 2"
      ]
    },
    {
      "metadata": {
        "id": "rjf4kzpR5mIo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wordseg_model = WordsegModel(dim_charvec=16, dim_trans=32, no_layers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XHLHW8c-ZFE_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The output of this model is a sequence of word delimiter vectors. These vectors contains two components:\n",
        "\n",
        "1. The log-probability of breaking\n",
        "2. The log-probability of not breaking"
      ]
    },
    {
      "metadata": {
        "id": "f-sNVWBf5mIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "25f1d09b-0338-4fd8-c0df-b23095bda681"
      },
      "cell_type": "code",
      "source": [
        "charidxs, wordbrks = sent2data(['my', 'name'])\n",
        "brkvecs = wordseg_model(charidxs)\n",
        "print(brkvecs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.6434, -0.7455],\n",
            "        [-0.6656, -0.7215],\n",
            "        [-0.6556, -0.7322],\n",
            "        [-0.6301, -0.7605],\n",
            "        [-0.6514, -0.7367],\n",
            "        [-0.7204, -0.6667]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-5AUXyf75mIq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Converting to a Sequence of Prediction Classes"
      ]
    },
    {
      "metadata": {
        "id": "531YKcgNZjSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Last we have to convert a sequence of word delimiters into a sequence of delimiter classes (`True` for breaking and `False` for not breaking)."
      ]
    },
    {
      "metadata": {
        "id": "a08K9x_K5mIr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def wordbrks2brkvec(wordbrks):\n",
        "    brkvec = T.LongTensor(len(wordbrks))\n",
        "    for i in range(len(wordbrks)):\n",
        "        if wordbrks[i]: brkvec[i] = 0\n",
        "        else: brkvec[i] = 1\n",
        "    return brkvec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pVoa2Bu_5mIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b87c36f5-fb6a-46a7-90f9-492f0934756b"
      },
      "cell_type": "code",
      "source": [
        "print(wordbrks)\n",
        "gold_brkvec = wordbrks2brkvec(wordbrks)\n",
        "print(gold_brkvec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False, True, False, False, False, True]\n",
            "tensor([1, 0, 1, 1, 1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pLaNL-Md5mIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Negative Log Loss"
      ]
    },
    {
      "metadata": {
        "id": "vPYB1IYIZ05s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Negative Log Loss (NLL) is a loss function that shows how far the probability of the desired class is from 1. Let $\\mathbf{y}$ be a gold-standard vector and $\\mathbf{y}'$ be a predicted vector. Each $c$-th element of these vectors belongs to each class.\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "  \\mathrm{NLL} & = & - \\sum_{c=1}^M y_{c} \\log y'_{c}\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "The negative log loss outperforms the mean squared error (MSE) in the case where the sizes of $\\mathbf{y}$ and $\\mathbf{y}'$ are very large. In MSE, we have to compute the difference between each pair of elements in both vectors. In NLL, however, we compute the difference only for the desired class, therefore boosting the speed."
      ]
    },
    {
      "metadata": {
        "id": "Rk8c0hal5mIt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b232546a-999a-47ee-d425-47cbbcf0c9a0"
      },
      "cell_type": "code",
      "source": [
        "loss_fn = N.NLLLoss()\n",
        "loss_fn(brkvecs, gold_brkvec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7268, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "kBPoyuxJ5mIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-------"
      ]
    },
    {
      "metadata": {
        "id": "xHhtyxm55mIw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Procedure"
      ]
    },
    {
      "metadata": {
        "id": "B0q9nGIbcvi5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let just imitate the training procedure of Lab 2. :P"
      ]
    },
    {
      "metadata": {
        "id": "KdiYkSqy5mIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(wordseg_model, training_data, epochs, loss_fn, optimizer):\n",
        "    no_samples = len(training_data)\n",
        "    loss_history = []\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        random.shuffle(training_data)\n",
        "        \n",
        "        for (charidxs, wordbrks) in tqdm(training_data):                \n",
        "            pred_brkvecs = wordseg_model(charidxs)       # Perform prediction\n",
        "            gold_brkvec = wordbrks2brkvec(wordbrks)      # Gold standard\n",
        "            \n",
        "            loss = loss_fn(pred_brkvecs, gold_brkvec)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            optimizer.zero_grad()      # Clear gradient cache\n",
        "            loss.backward()            # Perform backpropagation\n",
        "            optimizer.step()           # Update the model parameters\n",
        "        \n",
        "        loss_history.append(total_loss / no_samples)\n",
        "        \n",
        "    # Plot the loss history with Matplotlib\n",
        "    epoch_count = range(1, epochs + 1)\n",
        "    plt.plot(epoch_count, loss_history, 'b--')\n",
        "    plt.legend(['Training Loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TnfTLa4Ac9N-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**PLEASE NOTE THAT TRAINING WORD SEGMENTATION IS TIME-CONSUMING.**</font>"
      ]
    },
    {
      "metadata": {
        "id": "SHrMlVd35mIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "10a4b909-e9b5-48d1-9b0e-df7d143daa35"
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "# epochs = 20      # Only if you can wait\n",
        "learning_rate = 0.001\n",
        "\n",
        "loss_fn = N.NLLLoss()\n",
        "optimizer = O.Adam(wordseg_model.parameters(), lr=learning_rate)\n",
        "train_model(wordseg_model, training_set, epochs, loss_fn, optimizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2484/2484 [03:07<00:00, 13.27it/s]\n",
            "100%|██████████| 2484/2484 [03:09<00:00, 16.88it/s]\n",
            "100%|██████████| 2484/2484 [03:08<00:00, 13.20it/s]\n",
            "100%|██████████| 2484/2484 [03:08<00:00, 13.15it/s]\n",
            "100%|██████████| 2484/2484 [03:07<00:00, 13.25it/s]\n",
            "100%|██████████| 2484/2484 [03:08<00:00, 10.73it/s]\n",
            "100%|██████████| 2484/2484 [03:08<00:00, 13.18it/s]\n",
            "100%|██████████| 2484/2484 [03:06<00:00, 18.64it/s]\n",
            "100%|██████████| 2484/2484 [03:07<00:00, 13.26it/s]\n",
            "100%|██████████| 2484/2484 [03:06<00:00, 13.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0652582128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IucYW7kq5mIz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-----"
      ]
    },
    {
      "metadata": {
        "id": "oQ_9KpAx5mI0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Trying out"
      ]
    },
    {
      "metadata": {
        "id": "7Fi2wdaT5mI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(wordseg_model, charseq):\n",
        "    charidxs = str2idxseq(charseq)\n",
        "    pred_brkvecs = wordseg_model(charidxs)\n",
        "    pred_wordbrks = []\n",
        "    for i in range(len(charidxs)):\n",
        "        pred_wordbrk = (pred_brkvecs[i][0] > pred_brkvecs[i][1])\n",
        "        # print(pred_wordbrk)\n",
        "        pred_wordbrks.append(pred_wordbrk)\n",
        "    \n",
        "    sent = []\n",
        "    word = []\n",
        "    begpos = 0\n",
        "    for i in range(len(pred_wordbrks)):\n",
        "        if pred_wordbrks[i]:\n",
        "            word.append(charseq[i])\n",
        "            sent.append(word)\n",
        "            word = []\n",
        "            begpos = i\n",
        "        else:\n",
        "            word.append(charseq[i])\n",
        "    if len(word) > 0: sent.append(word)\n",
        "        \n",
        "    return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWToNFVCGWuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e47b9651-5f0e-42bc-847f-26b7deb69a11"
      },
      "cell_type": "code",
      "source": [
        "words = tokenize(wordseg_model, 'tobe,ornottobe,thatisthequestion.')\n",
        "print(' '.join(''.join(word) for word in words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to be , or not to be , that is the question .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nQjIi42d5mI1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-------"
      ]
    },
    {
      "metadata": {
        "id": "Ov3YGt6G5mI1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing Procedure"
      ]
    },
    {
      "metadata": {
        "id": "-dR_5UQzdHBn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's test our model and how much accuracy we can achieve."
      ]
    },
    {
      "metadata": {
        "id": "XHt9EKHy5mI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(wordseg_model, testing_data):\n",
        "    no_correct = 0.0\n",
        "    no_goldbrks = 0.0\n",
        "    no_predbrks = 0.0\n",
        "\n",
        "    for (charidxs, gold_wordbrks) in tqdm(testing_data):\n",
        "        pred_brkvecs = wordseg_model(charidxs)\n",
        "        pred_wordbrks = []\n",
        "        for i in range(len(charidxs)):\n",
        "            pred_wordbrk = (pred_brkvecs[i][0] > pred_brkvecs[i][1])\n",
        "            pred_wordbrks.append(pred_wordbrk)\n",
        "        \n",
        "        for i in range(len(charidxs)):\n",
        "            if gold_wordbrks[i] and gold_wordbrks[i] == pred_wordbrks[i]:\n",
        "                no_correct += 1.0\n",
        "            if gold_wordbrks[i]:\n",
        "                no_goldbrks += 1.0\n",
        "            if pred_wordbrks[i]:\n",
        "                no_predbrks += 1.0\n",
        "\n",
        "    precision = 100 * no_correct / no_predbrks\n",
        "    recall = 100 * no_correct / no_goldbrks\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    print('\\nPrecision = {}'.format(precision))\n",
        "    print('Recall = {}'.format(recall))\n",
        "    print('F1 = {}'.format(f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNBemgRQ5mI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9c6e81ec-8aa9-4d0c-946d-a593ef3cdb9e"
      },
      "cell_type": "code",
      "source": [
        "test_model(wordseg_model, testing_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 622/622 [00:17<00:00, 35.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Precision = 96.29794826048172\n",
            "Recall = 94.06041243101946\n",
            "F1 = 95.16602997355275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "u4WPeANWfAIW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ]
    },
    {
      "metadata": {
        "id": "wbCxsdeVelaw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vary the dimension of character vectors, the dimension of internal states, and the number of RNN layers. Observe the change of F1 scores.\n",
        "\n",
        "<font color=\"red\">**PLEASE NOTE THAT TRAINING WORD SEGMENTATION IS TIME-CONSUMING.**</font>"
      ]
    },
    {
      "metadata": {
        "id": "_nLw4RKh5mI4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for dim_charvec in [4, 8, 16, 32, 64]:\n",
        "#     for dim_trans in [4, 8, 16, 32, 64]:\n",
        "#         for no_layers in [1, 2, 3]:\n",
        "#             print('>>> dim_charvec={}  dim_trans={}  no_layers={}'.format(dim_charvec, dim_trans, no_layers))\n",
        "#             wordseg_model_ex1 = WordsegModel(dim_charvec, dim_trans, no_layers)\n",
        "#             train_model(wordseg_model_ex1, training_set, epochs, loss_fn, optimizer)\n",
        "#             test_model(wordseg_model_ex1, testing_set)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}